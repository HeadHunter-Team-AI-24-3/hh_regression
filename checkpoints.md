# 1. План проекта
## 1.1 Парсинг данных
В данном блоке нам необходимо собрать данные, которые в дальнейшем будут использованы для обучения нашей модели.
Для выполнения данной цели, необходимо выполнить следующий задачи:
1. Изучение документации HeadHunter api(https://api.hh.ru/openapi/redoc)
2. Изучение инструментов, с помощью которых будет происходить парсинг данных (Например: библотека requests)
3. Выбор регионов для парсинга вакансий (Россия и другие страны СНГ)
4. Анализ полей вакансий, которые мы хотим видеть в данных по результату парсинга
5. Парсинг данных

Для выполнения вышеперечилсенных задач необходимо:
* Выбор библиотеки для парсинга
* Написать код парсера данных

В итоге после выполнения поставленной цели, мы будем иметь в своем распоряжении, большой объем данных по вакансиям, с необходимыми на полями.
Это позовлит нам перейти к следующему этапу, а именно к "Подготовка данных для ML модели"
## 1.2 EDA & data preprocessing
В данном блоке мы планируем выполнить следующие задачи:
1. Анализ количества уникальных навыков, указанных в вакансии 
2. Распределение записей по категориям
3. Кластерный анализ
4. Анализ компаний 
4. Анализ средней зарплаы по категориям
5. Упорядочение навыков по убыванию среней зарплаты

Для выполнения вышеперечисленных задач необходимо провести:
* Очистку текста вакансий (удаление лишних символов, пробелов, стоп-слов).

В итоге, после успешного прохождения данного этапа мы сможем:
* Определение средней зарплаты для каждого навыка
* Определение навыков которые оказывают наибольшее влияние на уровень заработной платы.
## 1.3 Этап ML
1. Подготовка данных для ML модели
2. Попытка регрессии с помощью ML моделей: 
   - линейная регрессия (Scikit-learn LinearRegression)
   - дерево решений (Scikit-learn Decision Trees)
   - случайный лес (Scikit-learn RandomForestClassifier)
   - бустинги (XGBoost, Catboost, LightGBM)
   - SVM (Scikit-learn Support Vector Machines)
3. Подбор гиперпараметров (Оптимизация модели)
   - Методами библиотеки Scikit-learn
      - GridSearchCV
   - Optuna
   - Встроенные методы отимизации (XGBoost, Catboost, LightGBM)
4. Тестирование результатов

## 1.4 Этап DLКомпиляция модели
1. Дизайн архитектуры нейронной сети на PyTorch
2. Компиляция модели НС
3. Оптимизация гиперпараметров обучения
4. Пост-обучающая валидация и тестирование


## 1.5 Интеграция готовой модели в TelegramBot'а
1. Написание TG-бота (telebot)
2. Интеграция модели внутрь pipline бота
3. Раскатка сервиса
4. Тестирование сервиса

## 1.6 Что хотим добавить
1. Кластеризация новых вакансий
2. Попытка конвертации текста в эмбеддинги
3. Попытка конвертации категориальных данных в эмбеддинги
4. Помощьник навыков в TG-бота (по описанию вакансии составит план обучения)